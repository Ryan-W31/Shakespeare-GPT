{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bf0780a",
   "metadata": {},
   "source": [
    "# Shakespeare GPT -- GPT From Scratch!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "c5ccfe15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2457a2d",
   "metadata": {},
   "source": [
    "# Dataset Exploration and Modification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "ae8e07e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in input\n",
    "with open('./input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "b9816c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset in chars: 1115394\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# inpeact the length and the first 1000 chars\n",
    "print(\"Length of dataset in chars:\", len(text))\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "7e4b5800-4a33-435b-8701-b5327fd6e4d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# create an alphabet based on the input\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(chars)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "56a6dae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 43, 50, 50, 53, 1, 35, 53, 56, 50, 42, 2]\n",
      "Hello World!\n"
     ]
    }
   ],
   "source": [
    "# time to tokenize the alphabet (character tokenizer)\n",
    "\n",
    "# lets create a mapping for encoding (char to int) and decoding (int to char)\n",
    "stoi = {s:i for i,s in enumerate(chars)}\n",
    "itos = {i:s for s, i in stoi.items()}\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s] # string to list of ints\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # list of ints to string\n",
    "\n",
    "print(encode(\"Hello World!\"))\n",
    "print(decode(encode(\"Hello World!\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "3ecfc684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450612a0",
   "metadata": {},
   "source": [
    "# Dataset Splitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "1857b404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: torch.Size([1003854])\n",
      "VAL: torch.Size([111540])\n"
     ]
    }
   ],
   "source": [
    "# splitting the dataset into a train (90%) set and a validation (10%) set\n",
    "n = int(0.9 * len(data))\n",
    "train = data[:n]\n",
    "val = data[n:]\n",
    "\n",
    "print(\"TRAIN:\", train.shape)\n",
    "print(\"VAL:\", val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "6ae21549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs shape: torch.Size([4, 8])\n",
      "Inputs:\n",
      "tensor([[51, 47, 53,  6,  1, 58, 46, 53],\n",
      "        [37, 30, 30, 17, 24, 10,  0, 21],\n",
      "        [21,  5, 50, 50,  1, 54, 56, 53],\n",
      "        [41, 43, 47, 60, 43,  0, 37, 53]])\n",
      "\n",
      "Targets shape: torch.Size([4, 8])\n",
      "Targets:\n",
      "\n",
      "tensor([[47, 53,  6,  1, 58, 46, 53, 59],\n",
      "        [30, 30, 17, 24, 10,  0, 21,  1],\n",
      "        [ 5, 50, 50,  1, 54, 56, 53, 51],\n",
      "        [43, 47, 60, 43,  0, 37, 53, 59]])\n",
      "\n",
      "Block #1:\n",
      "[51] ===> 47\n",
      "[51, 47] ===> 53\n",
      "[51, 47, 53] ===> 6\n",
      "[51, 47, 53, 6] ===> 1\n",
      "[51, 47, 53, 6, 1] ===> 58\n",
      "[51, 47, 53, 6, 1, 58] ===> 46\n",
      "[51, 47, 53, 6, 1, 58, 46] ===> 53\n",
      "[51, 47, 53, 6, 1, 58, 46, 53] ===> 59\n",
      "\n",
      "Block #2:\n",
      "[37] ===> 30\n",
      "[37, 30] ===> 30\n",
      "[37, 30, 30] ===> 17\n",
      "[37, 30, 30, 17] ===> 24\n",
      "[37, 30, 30, 17, 24] ===> 10\n",
      "[37, 30, 30, 17, 24, 10] ===> 0\n",
      "[37, 30, 30, 17, 24, 10, 0] ===> 21\n",
      "[37, 30, 30, 17, 24, 10, 0, 21] ===> 1\n",
      "\n",
      "Block #3:\n",
      "[21] ===> 5\n",
      "[21, 5] ===> 50\n",
      "[21, 5, 50] ===> 50\n",
      "[21, 5, 50, 50] ===> 1\n",
      "[21, 5, 50, 50, 1] ===> 54\n",
      "[21, 5, 50, 50, 1, 54] ===> 56\n",
      "[21, 5, 50, 50, 1, 54, 56] ===> 53\n",
      "[21, 5, 50, 50, 1, 54, 56, 53] ===> 51\n",
      "\n",
      "Block #4:\n",
      "[41] ===> 43\n",
      "[41, 43] ===> 47\n",
      "[41, 43, 47] ===> 60\n",
      "[41, 43, 47, 60] ===> 43\n",
      "[41, 43, 47, 60, 43] ===> 0\n",
      "[41, 43, 47, 60, 43, 0] ===> 37\n",
      "[41, 43, 47, 60, 43, 0, 37] ===> 53\n",
      "[41, 43, 47, 60, 43, 0, 37, 53] ===> 59\n"
     ]
    }
   ],
   "source": [
    "# creating a batch with visualization\n",
    "\n",
    "torch.manual_seed(314159365) # for reproducibility\n",
    "batch_size = 4 # number of blocks in each batche\n",
    "block_size = 8 # context length\n",
    "\n",
    "def get_batch(split):\n",
    "    # create a batch of inputs (x) and targets (y)\n",
    "    \n",
    "    data = train if split == 'train' else val # choose split to use\n",
    "    \n",
    "    idx = torch.randint(len(data) - block_size, (batch_size,)) # get index of random token\n",
    "        \n",
    "    # get inputs and targets based in index\n",
    "    x = torch.stack([data[i : i + block_size] for i in idx])\n",
    "    y = torch.stack([data[i + 1 : i + block_size + 1] for i in idx])\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "Xb, Yb = get_batch('train')\n",
    "print(\"Inputs shape:\", Xb.shape)\n",
    "print(\"Inputs:\")\n",
    "print(Xb)\n",
    "print(\"\\nTargets shape:\", Yb.shape)\n",
    "print(\"Targets:\\n\")\n",
    "print(Yb)\n",
    "\n",
    "for b in range(batch_size):\n",
    "    print(f'\\nBlock #{b+1}:')\n",
    "    for i in range(block_size):\n",
    "        context = Xb[b, :i+1] # context 'slides' over input\n",
    "        target = Yb[b, i]\n",
    "        print(f'{context.tolist()} ===> {target}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78df4ca0",
   "metadata": {},
   "source": [
    "# Bigram Language Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "30a22d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "4.797330856323242\n",
      "Expected loss: 4.174387454986572\n",
      "\n",
      "MtcZK!kHeruAiysl3sI\n",
      "&\n",
      "aalhb$GxSyyysk3RkdWF?Yk\n",
      "&iqfF?oHRwm?cqAZxSb  eU$WkqZlD.gnN-zYcjoduqpR!NPZtqjjA\n"
     ]
    }
   ],
   "source": [
    "# lets start with a very simple language model: the Bigram\n",
    "# we will use PyTorch modules rather than building it ourselves\n",
    "# (see my makemore-CityNames repo for Bigram and Trigram models from scratch)\n",
    "\n",
    "torch.manual_seed(314159265) # fro reproducibility\n",
    "\n",
    "class Bigram(nn.Module):\n",
    "    \n",
    "    # initialize the model\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size) # create an 2D embedding table\n",
    "    \n",
    "    # forward pass\n",
    "    def forward(self, idx, targets=None):\n",
    "        \n",
    "        # idx and targets are a both (B, T) tensors of integers where B = batch and T = time (or block)\n",
    "        logits = self.token_embedding_table(idx) # (B, T, C) | C = channels (or vocab_size)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # dimension manipulation\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view((B*T, C))\n",
    "            targets = targets.view((B*T))\n",
    "            loss = F.cross_entropy(logits, targets) # get loss using cross_entropy\n",
    "        return logits, loss\n",
    "    \n",
    "    # generate new tokens\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is a (B, T) tensor\n",
    "        for _ in range(max_new_tokens):\n",
    "            \n",
    "            logits, loss = self(idx) # get predictions\n",
    "            \n",
    "            logits = logits[:, -1, :] # focus on last time step (B, C)\n",
    "            probs = F.softmax(logits, dim=1) # get probabilities over rows (B, C)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # sample from probs distribution (B, 1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # concatenate new token (B, T+1)\n",
    "        \n",
    "        return idx\n",
    "    \n",
    "m = Bigram(vocab_size)\n",
    "logits, loss = m(Xb, Yb)\n",
    "print(logits.shape)\n",
    "print(loss.item())\n",
    "print(\"Expected loss:\", -torch.log(torch.tensor([1/65])).item())\n",
    "\n",
    "# lets generate tokens from the Bigram\n",
    "idx = torch.zeros((1,1), dtype=torch.long) # get index (first token is index 0 or '/n')\n",
    "out = m.generate(idx, max_new_tokens=100)[0].tolist() # generate the new tokens\n",
    "print(decode(out)) # decode the tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee97193",
   "metadata": {},
   "source": [
    "### Yay, garbage! But thats ok because the Bigram model is random right now... so let's train it!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "dd7cec22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer (to see an optimizer (gradient descent) from scratch go to my makemore-CityNames repo)\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "2fc41d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.5410284996032715\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for steps in range(100):\n",
    "    Xb, Yb = get_batch('train') # get sample batch\n",
    "    \n",
    "    # evaluate loss\n",
    "    logits, loss = m(Xb, Yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "313b40c2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DA.WfRiSTiw.Ovs-ezvjeXZMv VNTaxgjFpEPG$YFcBp'oEdbZBHEJzj .vSb\n",
      "Qe CMuavoEgI?lL' GLsFyUBFp.mhVS.Op'&$fSvXTEXFDPpRgP-oe'aF,vlPYOIqbwFfQTEkbruXs-rh&3sFRxsMGM3ND.-UGHJkLQPUz;&ISKblGsNs!yM,PG!kcPYZSNkomOeXFrEdDkr;;xhcvGgnyj n&hpjKueskstDuGGIFhUBzOq?MsNT3v Hv'MA'TovSwIuZb ksv'kxy!:;KWzvS;\n",
      "UO!keKWRiBH?RqDRO\n"
     ]
    }
   ],
   "source": [
    "# lets generate tokens from the trained Bigram\n",
    "idx = torch.zeros((1,1), dtype=torch.long) # get index (first token is index 0 or '/n')\n",
    "out = m.generate(idx, max_new_tokens=300)[0].tolist() # generate the new tokens\n",
    "print(decode(out)) # decode the tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2b59e4",
   "metadata": {},
   "source": [
    "# The Math Behind Self-Attention !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "321e5bb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# consider the following:\n",
    "\n",
    "torch.manual_seed(314159265)\n",
    "\n",
    "B, T, C = 4, 8, 2 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "1def4029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 1: inefficient approach\n",
    "\n",
    "# we want x[b, t] = mean(x[b,i]) where i <= t\n",
    "x_bow = torch.zeros((B,T,C)) # bow: bag of words\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1] # (t, C)\n",
    "        x_bow[b,t] = torch.mean(xprev, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "fee43d03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 2: efficient approach but not what we want yet\n",
    "\n",
    "weights = torch.tril(torch.ones((T,T))) # get trianglular ones tensor\n",
    "weights = weights / torch.sum(weights, 1, keepdims=True) # convert ones to distributions\n",
    "mat = weights @ x  # matrix multiplication\n",
    "\n",
    "torch.allclose(x_bow,mat) # evaluates to true if all values are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "b2732459",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 3: efficient and uses Softmax\n",
    "tril = torch.tril(torch.ones((T,T)))\n",
    "weights = torch.zeros((T,T)) # get trianglular ones tensor\n",
    "weights = weights.masked_fill(tril == 0, float('-inf'))\n",
    "weights = F.softmax(weights, dim=1)\n",
    "mat = weights @ x  # matrix multiplication\n",
    "\n",
    "torch.allclose(x_bow,mat) # evaluates to true if all values are the same"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ed9a33",
   "metadata": {},
   "source": [
    "### As we can see, we can use matrix multiplication to optimize the calculation of mean over the time steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "5eda71ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 4: Attention!\n",
    "torch.manual_seed(314159265)\n",
    "\n",
    "B, T, C = 4, 8, 32\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "# a single Head of self-attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False) # token we are at\n",
    "query = nn.Linear(C, head_size, bias=False) # past tokens we want context from\n",
    "value = nn.Linear(C, head_size, bias=False) # communicates data if interested\n",
    "\n",
    "k = key(x) # (B, T, head_size) | in cross attention, this would come from somewhere else\n",
    "q = query(x) # (B, T, head_size)\n",
    "weights = q @ k.transpose(-2, -1) # (B, T, head_size) @ (B, head_size, T) ===> (B, T, T) \n",
    "\n",
    "tril = torch.tril(torch.ones((T,T)))\n",
    "weights = weights.masked_fill(tril == 0, float('-inf')) # delete for encoder attention blocks\n",
    "weights = F.softmax(weights, dim=-1)\n",
    "\n",
    "v = value(x) # in cross attention, this would come from somewhere else\n",
    "out = weights @ v  # matrix multiplication\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adda3a91",
   "metadata": {},
   "source": [
    "#### Notes:\n",
    "\n",
    "- Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
    "- No notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode of tokens.\n",
    "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other.\n",
    "- _Self-Attention_ just means that the keys and values are produced from the same source as queries. In _Cross-Attention_, the queries get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
    "- In an _encoder_ attention block, just delete the single line that does the masking with `tril`, allowing all tokens to communicate. The example above is a _decoder_ attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
    "- _Scaled_ attention additionally divides `weights` by 1 / sqrt(head_size). This makes it so when Q, K are unit variance, `weights` will be unit variance too and Softmax will stay diffuse and not saturate too much i.e. softmax will start to act like one-hot encoding. Example below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "e8d2261f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k var: 1.1114 | q var: 1.1233 | weights var: 1.0711\n"
     ]
    }
   ],
   "source": [
    "# Scaled attention\n",
    "k = torch.randn(B,T,head_size)\n",
    "q = torch.randn(B,T,head_size)\n",
    "weights = q @ k.transpose(-2, -1) * head_size**-0.5 # scaled by head_size**-0.5\n",
    "print(f'k var: {k.var().item():.4f} | q var: {q.var().item():.4f} | weights var: {weights.var().item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f15b7f",
   "metadata": {},
   "source": [
    "# Tokenization!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48735184",
   "metadata": {},
   "source": [
    "#### Disclaimer\n",
    "Tokenization is a very, very important to LLMs. It should not be ignored! Maybe one day we won't need it, but we need it right now.\\\n",
    "Check out [this website](https://tiktokenizer.vercel.app) for a visual representation of tokenization!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ae4a8c",
   "metadata": {},
   "source": [
    "## Byte Pair Encoding Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "e0572375",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[71, 111, 111, 100, 32, 109, 111, 114, 110, 105, 110, 103, 33, 32, 128522]"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ord(x) for x in 'Good morning! üòä'] # getting unicode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "aa1c497b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[71,\n",
       " 111,\n",
       " 111,\n",
       " 100,\n",
       " 32,\n",
       " 109,\n",
       " 111,\n",
       " 114,\n",
       " 110,\n",
       " 105,\n",
       " 110,\n",
       " 103,\n",
       " 33,\n",
       " 32,\n",
       " 240,\n",
       " 159,\n",
       " 152,\n",
       " 138]"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list('Good morning! üòä'.encode('utf-8')) # getting utf-8 encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "af12f242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "ÔºµÔΩéÔΩâÔΩÉÔΩèÔΩÑÔΩÖ! üÖ§üÖùüÖòüÖíüÖûüÖìüÖî‚ÄΩ üá∫‚Äåüá≥‚ÄåüáÆ‚Äåüá®‚Äåüá¥‚Äåüá©‚Äåüá™! üòÑ The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to ‚Äúsupport Unicode‚Äù in our software (whatever that means‚Äîlike using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I don‚Äôt blame programmers for still finding the whole thing mysterious, even 30 years after Unicode‚Äôs inception.\n",
      "Text length: 1115394\n",
      "---\n",
      "[239, 188, 181, 239, 189, 142, 239, 189, 137, 239, 189, 131, 239, 189, 143, 239, 189, 132, 239, 189, 133, 33, 32, 240, 159, 133, 164, 240, 159, 133, 157, 240, 159, 133, 152, 240, 159, 133, 146, 240, 159, 133, 158, 240, 159, 133, 147, 240, 159, 133, 148, 226, 128, 189, 32, 240, 159, 135, 186, 226, 128, 140, 240, 159, 135, 179, 226, 128, 140, 240, 159, 135, 174, 226, 128, 140, 240, 159, 135, 168, 226, 128, 140, 240, 159, 135, 180, 226, 128, 140, 240, 159, 135, 169, 226, 128, 140, 240, 159, 135, 170, 33, 32, 240, 159, 152, 132, 32, 84, 104, 101, 32, 118, 101, 114, 121, 32, 110, 97, 109, 101, 32, 115, 116, 114, 105, 107, 101, 115, 32, 102, 101, 97, 114, 32, 97, 110, 100, 32, 97, 119, 101, 32, 105, 110, 116, 111, 32, 116, 104, 101, 32, 104, 101, 97, 114, 116, 115, 32, 111, 102, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 119, 111, 114, 108, 100, 119, 105, 100, 101, 46, 32, 87, 101, 32, 97, 108, 108, 32, 107, 110, 111, 119, 32, 119, 101, 32, 111, 117, 103, 104, 116, 32, 116, 111, 32, 226, 128, 156, 115, 117, 112, 112, 111, 114, 116, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 157, 32, 105, 110, 32, 111, 117, 114, 32, 115, 111, 102, 116, 119, 97, 114, 101, 32, 40, 119, 104, 97, 116, 101, 118, 101, 114, 32, 116, 104, 97, 116, 32, 109, 101, 97, 110, 115, 226, 128, 148, 108, 105, 107, 101, 32, 117, 115, 105, 110, 103, 32, 119, 99, 104, 97, 114, 95, 116, 32, 102, 111, 114, 32, 97, 108, 108, 32, 116, 104, 101, 32, 115, 116, 114, 105, 110, 103, 115, 44, 32, 114, 105, 103, 104, 116, 63, 41, 46, 32, 66, 117, 116, 32, 85, 110, 105, 99, 111, 100, 101, 32, 99, 97, 110, 32, 98, 101, 32, 97, 98, 115, 116, 114, 117, 115, 101, 44, 32, 97, 110, 100, 32, 100, 105, 118, 105, 110, 103, 32, 105, 110, 116, 111, 32, 116, 104, 101, 32, 116, 104, 111, 117, 115, 97, 110, 100, 45, 112, 97, 103, 101, 32, 85, 110, 105, 99, 111, 100, 101, 32, 83, 116, 97, 110, 100, 97, 114, 100, 32, 112, 108, 117, 115, 32, 105, 116, 115, 32, 100, 111, 122, 101, 110, 115, 32, 111, 102, 32, 115, 117, 112, 112, 108, 101, 109, 101, 110, 116, 97, 114, 121, 32, 97, 110, 110, 101, 120, 101, 115, 44, 32, 114, 101, 112, 111, 114, 116, 115, 44, 32, 97, 110, 100, 32, 110, 111, 116, 101, 115, 32, 99, 97, 110, 32, 98, 101, 32, 109, 111, 114, 101, 32, 116, 104, 97, 110, 32, 97, 32, 108, 105, 116, 116, 108, 101, 32, 105, 110, 116, 105, 109, 105, 100, 97, 116, 105, 110, 103, 46, 32, 73, 32, 100, 111, 110, 226, 128, 153, 116, 32, 98, 108, 97, 109, 101, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 102, 111, 114, 32, 115, 116, 105, 108, 108, 32, 102, 105, 110, 100, 105, 110, 103, 32, 116, 104, 101, 32, 119, 104, 111, 108, 101, 32, 116, 104, 105, 110, 103, 32, 109, 121, 115, 116, 101, 114, 105, 111, 117, 115, 44, 32, 101, 118, 101, 110, 32, 51, 48, 32, 121, 101, 97, 114, 115, 32, 97, 102, 116, 101, 114, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 153, 115, 32, 105, 110, 99, 101, 112, 116, 105, 111, 110, 46]\n",
      "Encoding length: 616\n"
     ]
    }
   ],
   "source": [
    "# Byte Pair Encoding\n",
    "# text copied from https://www.reedbeta.com/blog/programmers-intro-to-unicode/\n",
    "text2 = \"ÔºµÔΩéÔΩâÔΩÉÔΩèÔΩÑÔΩÖ! üÖ§üÖùüÖòüÖíüÖûüÖìüÖî‚ÄΩ üá∫‚Äåüá≥‚ÄåüáÆ‚Äåüá®‚Äåüá¥‚Äåüá©‚Äåüá™! üòÑ The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to ‚Äúsupport Unicode‚Äù in our software (whatever that means‚Äîlike using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I don‚Äôt blame programmers for still finding the whole thing mysterious, even 30 years after Unicode‚Äôs inception.\"\n",
    "tokens = text2.encode(\"utf-8\")\n",
    "tokens = list(map(int, tokens))\n",
    "print('---')\n",
    "print(text2)\n",
    "print(\"Text length:\", len(text))\n",
    "print('---')\n",
    "print(tokens)\n",
    "print(\"Encoding length:\", len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "f2a4013b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(20, (101, 32)), (15, (240, 159)), (12, (226, 128)), (12, (105, 110)), (10, (115, 32)), (10, (97, 110)), (10, (32, 97)), (9, (32, 116)), (8, (116, 104)), (7, (159, 135)), (7, (159, 133)), (7, (97, 114)), (6, (239, 189)), (6, (140, 240)), (6, (128, 140)), (6, (116, 32)), (6, (114, 32)), (6, (111, 114)), (6, (110, 103)), (6, (110, 100)), (6, (109, 101)), (6, (104, 101)), (6, (101, 114)), (6, (32, 105)), (5, (117, 115)), (5, (115, 116)), (5, (110, 32)), (5, (100, 101)), (5, (44, 32)), (5, (32, 115)), (4, (116, 105)), (4, (116, 101)), (4, (115, 44)), (4, (114, 105)), (4, (111, 117)), (4, (111, 100)), (4, (110, 116)), (4, (110, 105)), (4, (105, 99)), (4, (104, 97)), (4, (103, 32)), (4, (101, 97)), (4, (100, 32)), (4, (99, 111)), (4, (97, 109)), (4, (85, 110)), (4, (32, 119)), (4, (32, 111)), (4, (32, 102)), (4, (32, 85)), (3, (118, 101)), (3, (116, 115)), (3, (116, 114)), (3, (116, 111)), (3, (114, 116)), (3, (114, 115)), (3, (114, 101)), (3, (111, 102)), (3, (111, 32)), (3, (108, 108)), (3, (108, 101)), (3, (108, 32)), (3, (101, 115)), (3, (101, 110)), (3, (97, 116)), (3, (46, 32)), (3, (32, 240)), (3, (32, 112)), (3, (32, 109)), (3, (32, 100)), (3, (32, 98)), (2, (128, 153)), (2, (121, 32)), (2, (119, 104)), (2, (119, 101)), (2, (117, 112)), (2, (116, 97)), (2, (115, 117)), (2, (114, 121)), (2, (114, 111)), (2, (114, 97)), (2, (112, 114)), (2, (112, 112)), (2, (112, 111)), (2, (112, 108)), (2, (111, 110)), (2, (111, 103)), (2, (110, 115)), (2, (110, 111)), (2, (109, 109)), (2, (108, 105)), (2, (107, 101)), (2, (105, 116)), (2, (105, 111)), (2, (105, 107)), (2, (105, 100)), (2, (104, 116)), (2, (104, 111)), (2, (103, 114)), (2, (103, 104)), (2, (102, 116)), (2, (102, 111)), (2, (102, 32)), (2, (101, 226)), (2, (101, 118)), (2, (101, 112)), (2, (100, 111)), (2, (100, 105)), (2, (100, 97)), (2, (99, 97)), (2, (98, 101)), (2, (97, 108)), (2, (33, 32)), (2, (32, 114)), (2, (32, 110)), (2, (32, 99)), (1, (239, 188)), (1, (189, 143)), (1, (189, 142)), (1, (189, 137)), (1, (189, 133)), (1, (189, 132)), (1, (189, 131)), (1, (189, 32)), (1, (188, 181)), (1, (186, 226)), (1, (181, 239)), (1, (180, 226)), (1, (179, 226)), (1, (174, 226)), (1, (170, 33)), (1, (169, 226)), (1, (168, 226)), (1, (164, 240)), (1, (159, 152)), (1, (158, 240)), (1, (157, 240)), (1, (157, 32)), (1, (156, 115)), (1, (153, 116)), (1, (153, 115)), (1, (152, 240)), (1, (152, 132)), (1, (148, 226)), (1, (148, 108)), (1, (147, 240)), (1, (146, 240)), (1, (143, 239)), (1, (142, 239)), (1, (137, 239)), (1, (135, 186)), (1, (135, 180)), (1, (135, 179)), (1, (135, 174)), (1, (135, 170)), (1, (135, 169)), (1, (135, 168)), (1, (133, 164)), (1, (133, 158)), (1, (133, 157)), (1, (133, 152)), (1, (133, 148)), (1, (133, 147)), (1, (133, 146)), (1, (133, 33)), (1, (132, 239)), (1, (132, 32)), (1, (131, 239)), (1, (128, 189)), (1, (128, 157)), (1, (128, 156)), (1, (128, 148)), (1, (122, 101)), (1, (121, 115)), (1, (121, 101)), (1, (120, 101)), (1, (119, 111)), (1, (119, 105)), (1, (119, 99)), (1, (119, 97)), (1, (119, 32)), (1, (118, 105)), (1, (117, 116)), (1, (117, 114)), (1, (117, 103)), (1, (116, 119)), (1, (116, 116)), (1, (116, 108)), (1, (116, 63)), (1, (115, 226)), (1, (115, 111)), (1, (115, 105)), (1, (115, 101)), (1, (115, 97)), (1, (114, 117)), (1, (114, 108)), (1, (114, 100)), (1, (114, 95)), (1, (112, 116)), (1, (112, 97)), (1, (111, 122)), (1, (111, 119)), (1, (111, 116)), (1, (111, 108)), (1, (110, 226)), (1, (110, 110)), (1, (110, 101)), (1, (110, 99)), (1, (110, 97)), (1, (110, 46)), (1, (109, 121)), (1, (109, 111)), (1, (109, 105)), (1, (108, 117)), (1, (108, 100)), (1, (108, 97)), (1, (107, 110)), (1, (105, 118)), (1, (105, 109)), (1, (105, 108)), (1, (105, 103)), (1, (104, 105)), (1, (103, 115)), (1, (103, 101)), (1, (103, 46)), (1, (102, 105)), (1, (102, 101)), (1, (101, 120)), (1, (101, 109)), (1, (101, 46)), (1, (101, 44)), (1, (100, 119)), (1, (100, 45)), (1, (99, 104)), (1, (99, 101)), (1, (98, 115)), (1, (98, 108)), (1, (97, 119)), (1, (97, 103)), (1, (97, 102)), (1, (97, 98)), (1, (97, 32)), (1, (95, 116)), (1, (87, 101)), (1, (84, 104)), (1, (83, 116)), (1, (73, 32)), (1, (66, 117)), (1, (63, 41)), (1, (51, 48)), (1, (48, 32)), (1, (45, 112)), (1, (41, 46)), (1, (40, 119)), (1, (32, 226)), (1, (32, 121)), (1, (32, 118)), (1, (32, 117)), (1, (32, 108)), (1, (32, 107)), (1, (32, 104)), (1, (32, 101)), (1, (32, 87)), (1, (32, 84)), (1, (32, 83)), (1, (32, 73)), (1, (32, 66)), (1, (32, 51)), (1, (32, 40))]\n"
     ]
    }
   ],
   "source": [
    "# let's get the counts of all the unique two character sequences in the paragraph above using the encodings\n",
    "def get_counts(ids):\n",
    "    counts = {}\n",
    "    for pair in zip(ids, ids[1:]): # iterating over consecutive elements\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "    \n",
    "counts = get_counts(tokens)\n",
    "print(sorted({(v, k) for k, v in counts.items()}, reverse=True)) # sort elements by value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "73aa5953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('e', ' ')"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(101), chr(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe89550",
   "metadata": {},
   "source": [
    "So the most common sequence of two characters in the paragraph is 'e' and ' '."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "d000b6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[239, 188, 181, 239, 189, 142, 239, 189, 137, 239, 189, 131, 239, 189, 143, 239, 189, 132, 239, 189, 133, 33, 32, 240, 159, 133, 164, 240, 159, 133, 157, 240, 159, 133, 152, 240, 159, 133, 146, 240, 159, 133, 158, 240, 159, 133, 147, 240, 159, 133, 148, 226, 128, 189, 32, 240, 159, 135, 186, 226, 128, 140, 240, 159, 135, 179, 226, 128, 140, 240, 159, 135, 174, 226, 128, 140, 240, 159, 135, 168, 226, 128, 140, 240, 159, 135, 180, 226, 128, 140, 240, 159, 135, 169, 226, 128, 140, 240, 159, 135, 170, 33, 32, 240, 159, 152, 132, 32, 84, 104, 256, 118, 101, 114, 121, 32, 110, 97, 109, 256, 115, 116, 114, 105, 107, 101, 115, 32, 102, 101, 97, 114, 32, 97, 110, 100, 32, 97, 119, 256, 105, 110, 116, 111, 32, 116, 104, 256, 104, 101, 97, 114, 116, 115, 32, 111, 102, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 119, 111, 114, 108, 100, 119, 105, 100, 101, 46, 32, 87, 256, 97, 108, 108, 32, 107, 110, 111, 119, 32, 119, 256, 111, 117, 103, 104, 116, 32, 116, 111, 32, 226, 128, 156, 115, 117, 112, 112, 111, 114, 116, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 157, 32, 105, 110, 32, 111, 117, 114, 32, 115, 111, 102, 116, 119, 97, 114, 256, 40, 119, 104, 97, 116, 101, 118, 101, 114, 32, 116, 104, 97, 116, 32, 109, 101, 97, 110, 115, 226, 128, 148, 108, 105, 107, 256, 117, 115, 105, 110, 103, 32, 119, 99, 104, 97, 114, 95, 116, 32, 102, 111, 114, 32, 97, 108, 108, 32, 116, 104, 256, 115, 116, 114, 105, 110, 103, 115, 44, 32, 114, 105, 103, 104, 116, 63, 41, 46, 32, 66, 117, 116, 32, 85, 110, 105, 99, 111, 100, 256, 99, 97, 110, 32, 98, 256, 97, 98, 115, 116, 114, 117, 115, 101, 44, 32, 97, 110, 100, 32, 100, 105, 118, 105, 110, 103, 32, 105, 110, 116, 111, 32, 116, 104, 256, 116, 104, 111, 117, 115, 97, 110, 100, 45, 112, 97, 103, 256, 85, 110, 105, 99, 111, 100, 256, 83, 116, 97, 110, 100, 97, 114, 100, 32, 112, 108, 117, 115, 32, 105, 116, 115, 32, 100, 111, 122, 101, 110, 115, 32, 111, 102, 32, 115, 117, 112, 112, 108, 101, 109, 101, 110, 116, 97, 114, 121, 32, 97, 110, 110, 101, 120, 101, 115, 44, 32, 114, 101, 112, 111, 114, 116, 115, 44, 32, 97, 110, 100, 32, 110, 111, 116, 101, 115, 32, 99, 97, 110, 32, 98, 256, 109, 111, 114, 256, 116, 104, 97, 110, 32, 97, 32, 108, 105, 116, 116, 108, 256, 105, 110, 116, 105, 109, 105, 100, 97, 116, 105, 110, 103, 46, 32, 73, 32, 100, 111, 110, 226, 128, 153, 116, 32, 98, 108, 97, 109, 256, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 102, 111, 114, 32, 115, 116, 105, 108, 108, 32, 102, 105, 110, 100, 105, 110, 103, 32, 116, 104, 256, 119, 104, 111, 108, 256, 116, 104, 105, 110, 103, 32, 109, 121, 115, 116, 101, 114, 105, 111, 117, 115, 44, 32, 101, 118, 101, 110, 32, 51, 48, 32, 121, 101, 97, 114, 115, 32, 97, 102, 116, 101, 114, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 153, 115, 32, 105, 110, 99, 101, 112, 116, 105, 111, 110, 46]\n",
      "Modified Length: 596\n"
     ]
    }
   ],
   "source": [
    "# lets make a function to replace this sequence with the int 256 \n",
    "# (256 because utf-8 goes to 255 and we need a new encoding)\n",
    "top_pair = max(counts, key=counts.get)\n",
    "\n",
    "def replace_seq(ids, pair, replacement):\n",
    "    i = 0\n",
    "    while i < len(ids) - 1: # iterating over n - 1 elements\n",
    "        if ids[i:i+len(pair)] == list(pair): # if the pair matches the current items\n",
    "            ids = ids[:i] + [replacement] + ids[i+2:] # replace the items\n",
    "        i += 1 # add one to i\n",
    "    return ids\n",
    "\n",
    "mod = replace_seq(tokens, top_pair, 256)\n",
    "print(mod)\n",
    "print(\"Modified Length:\", len(mod))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c1d670",
   "metadata": {},
   "source": [
    "Using the function we made, we can see that (101, 32) does not appear anymore and is now replaced by the single integer 256. Becuse the length was originally 616, and (101,32) appeared 20 times, we can verify the function works properly because the modified length is 596 which is 20 less that 616."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "c1255675",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pair    1:  (101, 32)   20 ===>  256\n",
      "Pair    2: (240, 159)   15 ===>  257\n",
      "Pair    3: (226, 128)   12 ===>  258\n",
      "Pair    4: (105, 110)   12 ===>  259\n",
      "Pair    5:  (115, 32)   10 ===>  260\n",
      "Pair    6:  (97, 110)   10 ===>  261\n",
      "Pair    7: (116, 104)    8 ===>  262\n",
      "Pair    8: (257, 133)    7 ===>  263\n",
      "Pair    9: (257, 135)    7 ===>  264\n",
      "Pair   10:  (97, 114)    7 ===>  265\n",
      "[239, 188, 181, 239, 189, 142, 239, 189, 137, 239, 189, 131, 239, 189, 143, 239, 189, 132, 239, 189, 133, 33, 32, 263, 164, 263, 157, 263, 152, 263, 146, 263, 158, 263, 147, 263, 148, 258, 189, 32, 264, 186, 258, 140, 264, 179, 258, 140, 264, 174, 258, 140, 264, 168, 258, 140, 264, 180, 258, 140, 264, 169, 258, 140, 264, 170, 33, 32, 257, 152, 132, 32, 84, 104, 256, 118, 101, 114, 121, 32, 110, 97, 109, 256, 115, 116, 114, 105, 107, 101, 260, 102, 101, 265, 32, 261, 100, 32, 97, 119, 256, 259, 116, 111, 32, 262, 256, 104, 101, 265, 116, 260, 111, 102, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 260, 119, 111, 114, 108, 100, 119, 105, 100, 101, 46, 32, 87, 256, 97, 108, 108, 32, 107, 110, 111, 119, 32, 119, 256, 111, 117, 103, 104, 116, 32, 116, 111, 32, 258, 156, 115, 117, 112, 112, 111, 114, 116, 32, 85, 110, 105, 99, 111, 100, 101, 258, 157, 32, 259, 32, 111, 117, 114, 32, 115, 111, 102, 116, 119, 265, 256, 40, 119, 104, 97, 116, 101, 118, 101, 114, 32, 262, 97, 116, 32, 109, 101, 261, 115, 258, 148, 108, 105, 107, 256, 117, 115, 259, 103, 32, 119, 99, 104, 265, 95, 116, 32, 102, 111, 114, 32, 97, 108, 108, 32, 262, 256, 115, 116, 114, 259, 103, 115, 44, 32, 114, 105, 103, 104, 116, 63, 41, 46, 32, 66, 117, 116, 32, 85, 110, 105, 99, 111, 100, 256, 99, 261, 32, 98, 256, 97, 98, 115, 116, 114, 117, 115, 101, 44, 32, 261, 100, 32, 100, 105, 118, 259, 103, 32, 259, 116, 111, 32, 262, 256, 262, 111, 117, 115, 261, 100, 45, 112, 97, 103, 256, 85, 110, 105, 99, 111, 100, 256, 83, 116, 261, 100, 265, 100, 32, 112, 108, 117, 260, 105, 116, 260, 100, 111, 122, 101, 110, 260, 111, 102, 32, 115, 117, 112, 112, 108, 101, 109, 101, 110, 116, 265, 121, 32, 261, 110, 101, 120, 101, 115, 44, 32, 114, 101, 112, 111, 114, 116, 115, 44, 32, 261, 100, 32, 110, 111, 116, 101, 260, 99, 261, 32, 98, 256, 109, 111, 114, 256, 262, 261, 32, 97, 32, 108, 105, 116, 116, 108, 256, 259, 116, 105, 109, 105, 100, 97, 116, 259, 103, 46, 32, 73, 32, 100, 111, 110, 258, 153, 116, 32, 98, 108, 97, 109, 256, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 260, 102, 111, 114, 32, 115, 116, 105, 108, 108, 32, 102, 259, 100, 259, 103, 32, 262, 256, 119, 104, 111, 108, 256, 262, 259, 103, 32, 109, 121, 115, 116, 101, 114, 105, 111, 117, 115, 44, 32, 101, 118, 101, 110, 32, 51, 48, 32, 121, 101, 265, 260, 97, 102, 116, 101, 114, 32, 85, 110, 105, 99, 111, 100, 101, 258, 153, 260, 259, 99, 101, 112, 116, 105, 111, 110, 46]\n",
      "token_bp length: 508\n"
     ]
    }
   ],
   "source": [
    "# now lets put both functions together into a larger function.\n",
    "# the amount of repititions is a hyperparameter we can tune\n",
    "\n",
    "\n",
    "def bp_enc(ids, iters):\n",
    "    merges = {}\n",
    "    tmp = list(ids)\n",
    "    s = 256\n",
    "    for i in range(iters):\n",
    "        counts = get_counts(tmp)\n",
    "        pair = max(counts, key=counts.get)\n",
    "        merges[s+i] = pair\n",
    "        print(f\"Pair {i+1:4d}: {str(pair):>10s} {counts.get(pair):4d} ===> {s + i:4d}\")\n",
    "        tmp = replace_seq(tmp, pair, s + i)\n",
    "    \n",
    "    return tmp, merges\n",
    "\n",
    "tokens_bp, merges = bp_enc(tokens, 10)\n",
    "print(tokens_bp)\n",
    "print(\"token_bp length:\", len(tokens_bp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e670fb23",
   "metadata": {},
   "source": [
    "The top ten pair counts sum up to 108. 616 - 108 = 508 so the function works as expected!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "7cd944a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = text[:20000] # first 10000 characters from tiny shakespeare\n",
    "tokens = sample.encode(\"utf-8\") # raw encoding bytes\n",
    "tokens = list(map(int, tokens)) # get ints in range 0-255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "d57451eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pair    1:  (101, 32)  517 ===>  256\n",
      "Pair    2: (116, 104)  402 ===>  257\n",
      "Pair    3:  (116, 32)  321 ===>  258\n",
      "Pair    4:  (115, 32)  291 ===>  259\n",
      "Pair    5: (111, 117)  270 ===>  260\n",
      "Pair    6:   (44, 32)  248 ===>  261\n",
      "Pair    7:  (100, 32)  234 ===>  262\n",
      "Pair    8:  (114, 32)  203 ===>  263\n",
      "Pair    9: (105, 110)  183 ===>  264\n",
      "Pair   10:  (97, 110)  170 ===>  265\n",
      "Pair   11: (101, 110)  167 ===>  266\n",
      "Pair   12:   (58, 10)  160 ===>  267\n",
      "Pair   13:  (121, 32)  147 ===>  268\n",
      "Pair   14:   (10, 10)  146 ===>  269\n",
      "Pair   15: (101, 114)  140 ===>  270\n",
      "Pair   16: (111, 110)  138 ===>  271\n",
      "Pair   17: (108, 108)  131 ===>  272\n",
      "Pair   18:  (97, 114)  126 ===>  273\n",
      "Pair   19: (257, 256)  126 ===>  274\n",
      "Pair   20: (121, 260)  124 ===>  275\n",
      "tokens Length: 20000\n",
      "ids Length: 15756\n",
      "Compression ratio: 1.27x\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 276\n",
    "num_merges = vocab_size - 256\n",
    "ids = list(tokens)\n",
    "\n",
    "ids, merges = bp_enc(ids, num_merges)\n",
    "print(\"tokens Length:\", len(tokens))\n",
    "print(\"ids Length:\", len(ids))\n",
    "print(f\"Compression ratio: {len(tokens) / len(ids):.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b96287",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "The tokenizer is a completely seperate, independent module from the LLM. It has it's own training dataset (which could be different from the LLM) on which it will train on vocabularu using the Byte Pair Encoding (BPE) Algorithm. Only later does the LLM actually recieve the tokens the tokenizer produces. This means the LLM never directly deals with any text, only tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c00296",
   "metadata": {},
   "source": [
    "### Encoding and Decoding Using A BPE Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d991e24c",
   "metadata": {},
   "source": [
    "#### Decoding\n",
    "Given a sequence of integers in the range [0, vocab_size], what is the text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "68affebd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pre-processing\n",
    "vocab = {idx: bytes([idx]) for idx in range(256)} # int to byte list mapping\n",
    "\n",
    "# adding merges to vocab mapping\n",
    "for idx, (p0, p1) in merges.items():\n",
    "    vocab[idx] = vocab[p0] + vocab[p1]\n",
    "    \n",
    "def decode(ids):\n",
    "    # given list if ids (integers), returns Python string\n",
    "    tokens = b\"\".join([vocab[idx] for idx in ids])\n",
    "    text = tokens.decode('utf-8', errors='replace')\n",
    "    return text\n",
    "\n",
    "output = decode(ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3dcd33",
   "metadata": {},
   "source": [
    "#### Encoding\n",
    "Given a string, what is the encoding in integers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "ff5c59cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[70, 105, 114, 115, 116, 32, 67, 105, 116, 105]\n"
     ]
    }
   ],
   "source": [
    "def encode(text):\n",
    "    tokens  = list(text.encode('utf-8'))\n",
    "    while len(tokens) >= 2:\n",
    "        counts = get_counts(tokens)\n",
    "                   \n",
    "        # for any pair in counts, look at merges and get the min count\n",
    "        pair = min(counts, key = lambda p: merges.get(p, float(\"inf\")))\n",
    "        if pair not in merges:\n",
    "            break\n",
    "        \n",
    "        idx = merges[pair]\n",
    "        tokens = bp_enc(tokens, pair, idx)\n",
    "    \n",
    "    return tokens\n",
    "    \n",
    "\n",
    "ids = encode(output)\n",
    "print(ids[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "8995500c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "text2 = decode(encode(text))\n",
    "print(text == text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "9d9f297b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "valtext = \"hello world! this is validation text for the tokenizer :)\"\n",
    "valtext2 = decode(encode(valtext))\n",
    "print(valtext == valtext2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1bef1c",
   "metadata": {},
   "source": [
    "### Yay! Byte Pair Encoding is implemented!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679d2a0b",
   "metadata": {},
   "source": [
    "### ...Now onto more complicated tokenization methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ac763e",
   "metadata": {},
   "source": [
    "### Forcing splits using regex patterns (like GPT2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "7106d6ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', \"'ve\", ' world', '123', ' how', \"'s\", ' are', '      ', ' you', '!', 'I', '!?', 'HOW', \"'\", 'S', '  ']\n"
     ]
    }
   ],
   "source": [
    "import regex as re\n",
    "gpt2pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll'd| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "\n",
    "print(re.findall(gpt2pat, \"Hello've world123 how's are       you!I!?HOW'S  \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38bf1a2",
   "metadata": {},
   "source": [
    "Notice how the regex referring to `'` only deal with lowercase letters... So `'s` will work, but `'S` will not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "843dee6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[220, 220, 220, 23748, 995, 10185]\n",
      "[262, 24748, 1917, 12340]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "# GPT-2 (does not merge spaces)\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "print(enc.encode(\"    hello world!!!\"))\n",
    "\n",
    "# GPT-4 (merges spaces)\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "print(enc.encode(\"    hello world!!!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992fd67b",
   "metadata": {},
   "source": [
    "### Special Tokens\n",
    "Tokens used as delimiters, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt (env)",
   "language": "python",
   "name": "gpt-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
